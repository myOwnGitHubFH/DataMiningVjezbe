{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec04b9f",
   "metadata": {},
   "source": [
    "Sentiment analiza nad large movie reviews datasetom</br></br>\n",
    "\n",
    "\n",
    "import os - za pristup fajlovaima</br>\n",
    "import numpy - za računske operacije, te određene strukture poput nizova</br>\n",
    "Tokenizer - pretvaranje teksta u nizove brojeva (uglavnom riječi koje se najčešće pojavljuju)</br>\n",
    "Pad_sequences - neuralna mreža očekuje od ulaznih nizova da budu iste veličine, ovim omogućavamo jednake veličine (kraći se popunjavaju nulama, duži se skraćuju)</br>\n",
    "\n",
    "Funkcija load_reviews učitava podatke iz dataseta</br>\n",
    "S obzirom da je dataset formatiran kao glavni folder u kojem se nalaze dva podfoldera jedan za trening drugi za testne podatke</br>\n",
    "U oba ova podfoldera imamao folder za pozitivne i negativne recenzije</br>\n",
    "U ovim podfolderima svaki review je sačuvan kao poseban .txt file</br>\n",
    "Potrebno je izvršiti ekstrakciju ovih podataka i smještanje u određene varijable kako bi mogli adekvatno nastaviti sa analizom</br></br>\n",
    "\n",
    "Tokenizer je zadan da izrši pretvaranje 20000 najčešće spomenutih riječi u recenztijama u numeričku vrijednost</br>\n",
    "Ovo se radi tako što u zavisnosti od količine ponavljanja riječ dobija indeks</br>\n",
    "Texts.to.sequence potom kreira nizove takve da svaka rikeč ima svoj indeks (Film: 5, je: 2, odličan: 1)</br></br>\n",
    "\n",
    "pad_sequences kao ranije navedeno sređuje ove nizove da budu iste veličine</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_reviews(directory):\n",
    "    texts, labels = [], []\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_path = os.path.join(directory, label_type)\n",
    "        label = 0 if label_type == 'neg' else 1\n",
    "        for fname in os.listdir(dir_path):\n",
    "            if fname.endswith(\".txt\"):\n",
    "                with open(os.path.join(dir_path, fname), encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_reviews('aclImdb/train')\n",
    "test_texts, test_labels = load_reviews('aclImdb/test')\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "x_test = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "\n",
    "maxlen = 300\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099d660",
   "metadata": {},
   "source": [
    "Ovaj dataset je došao sa svojim zadanim vokabularom te ocjenom sentimenta koji smo također iskoristili za poboljšanje modela</br>\n",
    ".vocab dakle sadrži ključne riječi</br>\n",
    "dok imbdEr.txt sadži ocjene sentimenta</br></br>\n",
    "\n",
    "vocab smještamo u niz kao i ocjene sentimenta</br></br>\n",
    "\n",
    "word_to_sentiment spaja određenju riječ s njenom ocjenom te je u biti kreiran dictionary</br></br>\n",
    "\n",
    "embedding vector je u biti niz feature-a svake riječi</br>\n",
    "embedding matrica je u biti embedding vector za svaku riječ u jednoj strukturi</br>\n",
    "U biti na kraju pomoću indeksiraih riječi i embedding matrice koja nam predstavlja težine dobijamo pripremljen skup za treniranje i analizu sentimenta</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'aclImdb/imdb.vocab'\n",
    "sentiment_path = 'aclImdb/imdbEr.txt'\n",
    "\n",
    "with open(vocab_path, encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f]\n",
    "\n",
    "sentiment_scores = np.loadtxt(sentiment_path)\n",
    "\n",
    "word_to_sentiment = {word: sentiment_scores[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "embedding_dim = 128\n",
    "embedding_matrix = np.zeros((20000, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= 20000:\n",
    "        continue\n",
    "    sentiment = word_to_sentiment.get(word, 0.0)\n",
    "    embedding_matrix[i] = np.full((embedding_dim,), sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training text: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's bette\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training text:\", train_texts[0][:500]) # temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2233d",
   "metadata": {},
   "source": [
    "Sequential - način slaganja neurona (linearno) jedni za drugim u nizu</br>\n",
    "Embedding sloj koji koristi prethodno pripremljene nizove i matricu, inicijalizira težine</br>\n",
    "Dakle umjesto da model sam podešava početne težine mi mu dajemo težine koje smo izvukli iz dataseta što nam daje puno bolje performanse</br>\n",
    "LSTM (Long-Short term memory) sloj procesira inpute iz prethodnog sloja</br>\n",
    "LSTM je u biti vrta neuronske mreže tj. rekuralne neuronske mreže (RNN)</br>\n",
    "Pogodan je za sentiment analizu zbog svoje efikasnoti za sekvencijalnim podacima tj. podacima koji ovise o kontekstu (o prethodnim i sljedećim članovima)</br>\n",
    "Izabran je jer se dataset sastoji od 50000 podataka, LSTM pokazuje svoju efikasnot uglavnom na većim datasetovima</br>\n",
    "Dropout parametri su podešeni da sprječe overfitting tj. prenaučenost modela</br>\n",
    "Dense sloj je u potpunosti spojen s prethodnim slojem te koristi sigmoidnu aktivacijsku funkciju da nam da vjerovatno</br>\n",
    "Rezultat teži 0 negativan sentiment</br>\n",
    "Rezultat teži 1 pozitivan sentiment</br>\n",
    "Loss je zadan na binary_crossentropy jer je efikasan za probleme binarne klasifikacije</br>\n",
    "U našem slučaju to su dvije klase pozitivan i negativan sentiment</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> (9.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,560,000\u001b[0m (9.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> (9.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,560,000\u001b[0m (9.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=embedding_dim,\n",
    "              input_length=maxlen, weights=[embedding_matrix],\n",
    "              trainable=True), \n",
    "    LSTM(128, dropout=0.4, recurrent_dropout=0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064f8e6",
   "metadata": {},
   "source": [
    "Early stopping se koristi kako bi se izbjegao overfitting, a također i za uštedu vremena</br>\n",
    "U biti model je podešen da radi na 10 epoha tj. da 10 puta prođe kroz podatke te podesi težine i parametre</br>\n",
    "Patience je 2 i na osnovu gubitka prekida trening i uzima slučaj najbolje epohe</br>\n",
    "Validation split provjerava da li naš model dobro generalizira te da li je model sklon overfittingu</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6533a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 486ms/step - accuracy: 0.7673 - loss: 0.4929 - val_accuracy: 0.6030 - val_loss: 0.8534\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 449ms/step - accuracy: 0.7932 - loss: 0.4515 - val_accuracy: 0.8134 - val_loss: 0.4829\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 460ms/step - accuracy: 0.8224 - loss: 0.4108 - val_accuracy: 0.7568 - val_loss: 0.5154\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 467ms/step - accuracy: 0.8521 - loss: 0.3439 - val_accuracy: 0.8158 - val_loss: 0.4367\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 459ms/step - accuracy: 0.8798 - loss: 0.2904 - val_accuracy: 0.8236 - val_loss: 0.4802\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 413ms/step - accuracy: 0.8983 - loss: 0.2599 - val_accuracy: 0.8072 - val_loss: 0.5021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21010ca1090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc2547",
   "metadata": {},
   "source": [
    "Ostatak koda korišten je za evaluaciju i testiranje modela na eksternim podacima</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641234e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.8993 - loss: 0.2469\n",
      "Test Accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1a71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text, tokenizer, maxlen=300):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=maxlen)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Predicted sentiment: Negative 😞 (score: 0.2343)\n"
     ]
    }
   ],
   "source": [
    "text = \"Movie was bad.\"\n",
    "input_data = preprocess_input(text, tokenizer)\n",
    "prediction = model.predict(input_data)[0][0]\n",
    "\n",
    "sentiment = \"Positive 😀\" if prediction >= 0.5 else \"Negative 😞\"\n",
    "print(f\"Predicted sentiment: {sentiment} (score: {prediction:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27118ff",
   "metadata": {},
   "source": [
    "model je sačuvcan za upotrebu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf30252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"sentiment_lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
