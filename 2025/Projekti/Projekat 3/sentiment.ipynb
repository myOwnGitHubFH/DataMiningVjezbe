{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec04b9f",
   "metadata": {},
   "source": [
    "Sentiment analiza nad large movie reviews datasetom</br></br>\n",
    "\n",
    "\n",
    "import os - za pristup fajlovaima</br>\n",
    "import numpy - za raÄunske operacije, te odreÄ‘ene strukture poput nizova</br>\n",
    "Tokenizer - pretvaranje teksta u nizove brojeva (uglavnom rijeÄi koje se najÄeÅ¡Ä‡e pojavljuju)</br>\n",
    "Pad_sequences - neuralna mreÅ¾a oÄekuje od ulaznih nizova da budu iste veliÄine, ovim omoguÄ‡avamo jednake veliÄine (kraÄ‡i se popunjavaju nulama, duÅ¾i se skraÄ‡uju)</br>\n",
    "\n",
    "Funkcija load_reviews uÄitava podatke iz dataseta</br>\n",
    "S obzirom da je dataset formatiran kao glavni folder u kojem se nalaze dva podfoldera jedan za trening drugi za testne podatke</br>\n",
    "U oba ova podfoldera imamao folder za pozitivne i negativne recenzije</br>\n",
    "U ovim podfolderima svaki review je saÄuvan kao poseban .txt file</br>\n",
    "Potrebno je izvrÅ¡iti ekstrakciju ovih podataka i smjeÅ¡tanje u odreÄ‘ene varijable kako bi mogli adekvatno nastaviti sa analizom</br></br>\n",
    "\n",
    "Tokenizer je zadan da izrÅ¡i pretvaranje 20000 najÄeÅ¡Ä‡e spomenutih rijeÄi u recenztijama u numeriÄku vrijednost</br>\n",
    "Ovo se radi tako Å¡to u zavisnosti od koliÄine ponavljanja rijeÄ dobija indeks</br>\n",
    "Texts.to.sequence potom kreira nizove takve da svaka rikeÄ ima svoj indeks (Film: 5, je: 2, odliÄan: 1)</br></br>\n",
    "\n",
    "pad_sequences kao ranije navedeno sreÄ‘uje ove nizove da budu iste veliÄine</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_reviews(directory):\n",
    "    texts, labels = [], []\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_path = os.path.join(directory, label_type)\n",
    "        label = 0 if label_type == 'neg' else 1\n",
    "        for fname in os.listdir(dir_path):\n",
    "            if fname.endswith(\".txt\"):\n",
    "                with open(os.path.join(dir_path, fname), encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "                    labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_reviews('aclImdb/train')\n",
    "test_texts, test_labels = load_reviews('aclImdb/test')\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "x_test = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "\n",
    "maxlen = 300\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38cc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099d660",
   "metadata": {},
   "source": [
    "Ovaj dataset je doÅ¡ao sa svojim zadanim vokabularom te ocjenom sentimenta koji smo takoÄ‘er iskoristili za poboljÅ¡anje modela</br>\n",
    ".vocab dakle sadrÅ¾i kljuÄne rijeÄi</br>\n",
    "dok imbdEr.txt sadÅ¾i ocjene sentimenta</br></br>\n",
    "\n",
    "vocab smjeÅ¡tamo u niz kao i ocjene sentimenta</br></br>\n",
    "\n",
    "word_to_sentiment spaja odreÄ‘enju rijeÄ s njenom ocjenom te je u biti kreiran dictionary</br></br>\n",
    "\n",
    "embedding vector je u biti niz feature-a svake rijeÄi</br>\n",
    "embedding matrica je u biti embedding vector za svaku rijeÄ u jednoj strukturi</br>\n",
    "U biti na kraju pomoÄ‡u indeksiraih rijeÄi i embedding matrice koja nam predstavlja teÅ¾ine dobijamo pripremljen skup za treniranje i analizu sentimenta</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'aclImdb/imdb.vocab'\n",
    "sentiment_path = 'aclImdb/imdbEr.txt'\n",
    "\n",
    "with open(vocab_path, encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f]\n",
    "\n",
    "sentiment_scores = np.loadtxt(sentiment_path)\n",
    "\n",
    "word_to_sentiment = {word: sentiment_scores[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "embedding_dim = 128\n",
    "embedding_matrix = np.zeros((20000, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= 20000:\n",
    "        continue\n",
    "    sentiment = word_to_sentiment.get(word, 0.0)\n",
    "    embedding_matrix[i] = np.full((embedding_dim,), sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c5b447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training text: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's bette\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training text:\", train_texts[0][:500]) # temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2233d",
   "metadata": {},
   "source": [
    "Sequential - naÄin slaganja neurona (linearno) jedni za drugim u nizu</br>\n",
    "Embedding sloj koji koristi prethodno pripremljene nizove i matricu, inicijalizira teÅ¾ine</br>\n",
    "Dakle umjesto da model sam podeÅ¡ava poÄetne teÅ¾ine mi mu dajemo teÅ¾ine koje smo izvukli iz dataseta Å¡to nam daje puno bolje performanse</br>\n",
    "LSTM (Long-Short term memory) sloj procesira inpute iz prethodnog sloja</br>\n",
    "LSTM je u biti vrta neuronske mreÅ¾e tj. rekuralne neuronske mreÅ¾e (RNN)</br>\n",
    "Pogodan je za sentiment analizu zbog svoje efikasnoti za sekvencijalnim podacima tj. podacima koji ovise o kontekstu (o prethodnim i sljedeÄ‡im Älanovima)</br>\n",
    "Izabran je jer se dataset sastoji od 50000 podataka, LSTM pokazuje svoju efikasnot uglavnom na veÄ‡im datasetovima</br>\n",
    "Dropout parametri su podeÅ¡eni da sprjeÄe overfitting tj. prenauÄenost modela</br>\n",
    "Dense sloj je u potpunosti spojen s prethodnim slojem te koristi sigmoidnu aktivacijsku funkciju da nam da vjerovatno</br>\n",
    "Rezultat teÅ¾i 0 negativan sentiment</br>\n",
    "Rezultat teÅ¾i 1 pozitivan sentiment</br>\n",
    "Loss je zadan na binary_crossentropy jer je efikasan za probleme binarne klasifikacije</br>\n",
    "U naÅ¡em sluÄaju to su dvije klase pozitivan i negativan sentiment</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a2958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚     \u001b[38;5;34m2,560,000\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> (9.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,560,000\u001b[0m (9.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> (9.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,560,000\u001b[0m (9.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=embedding_dim,\n",
    "              input_length=maxlen, weights=[embedding_matrix],\n",
    "              trainable=True), \n",
    "    LSTM(128, dropout=0.4, recurrent_dropout=0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064f8e6",
   "metadata": {},
   "source": [
    "Early stopping se koristi kako bi se izbjegao overfitting, a takoÄ‘er i za uÅ¡tedu vremena</br>\n",
    "U biti model je podeÅ¡en da radi na 10 epoha tj. da 10 puta proÄ‘e kroz podatke te podesi teÅ¾ine i parametre</br>\n",
    "Patience je 2 i na osnovu gubitka prekida trening i uzima sluÄaj najbolje epohe</br>\n",
    "Validation split provjerava da li naÅ¡ model dobro generalizira te da li je model sklon overfittingu</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6533a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 486ms/step - accuracy: 0.7673 - loss: 0.4929 - val_accuracy: 0.6030 - val_loss: 0.8534\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 449ms/step - accuracy: 0.7932 - loss: 0.4515 - val_accuracy: 0.8134 - val_loss: 0.4829\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 460ms/step - accuracy: 0.8224 - loss: 0.4108 - val_accuracy: 0.7568 - val_loss: 0.5154\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 467ms/step - accuracy: 0.8521 - loss: 0.3439 - val_accuracy: 0.8158 - val_loss: 0.4367\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 459ms/step - accuracy: 0.8798 - loss: 0.2904 - val_accuracy: 0.8236 - val_loss: 0.4802\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 413ms/step - accuracy: 0.8983 - loss: 0.2599 - val_accuracy: 0.8072 - val_loss: 0.5021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21010ca1090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc2547",
   "metadata": {},
   "source": [
    "Ostatak koda koriÅ¡ten je za evaluaciju i testiranje modela na eksternim podacima</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641234e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.8993 - loss: 0.2469\n",
      "Test Accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e1a71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text, tokenizer, maxlen=300):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=maxlen)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Predicted sentiment: Negative ğŸ˜ (score: 0.2343)\n"
     ]
    }
   ],
   "source": [
    "text = \"Movie was bad.\"\n",
    "input_data = preprocess_input(text, tokenizer)\n",
    "prediction = model.predict(input_data)[0][0]\n",
    "\n",
    "sentiment = \"Positive ğŸ˜€\" if prediction >= 0.5 else \"Negative ğŸ˜\"\n",
    "print(f\"Predicted sentiment: {sentiment} (score: {prediction:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27118ff",
   "metadata": {},
   "source": [
    "model je saÄuvcan za upotrebu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf30252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"sentiment_lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
